{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMjF6QZN860nXE+5A89XlaK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NazninNaharSr/DeepLearning/blob/main/Neural_network_with_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data\n",
        "The first step is to define the functions and classes you intend to use in this post. You will use the NumPy library to load your dataset and PyTorch library for deep learning models."
      ],
      "metadata": {
        "id": "je24XPqXpsGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "jLWpkqHeq2UZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# load the dataset, split into input (X) and output (y) variables\n"
      ],
      "metadata": {
        "id": "od37opzHq-Hj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = np.loadtxt('/content/pima-indians-diabetes.data.csv', delimiter=',')\n",
        "X = dataset[:,0:8]\n",
        "y = dataset[:,8]"
      ],
      "metadata": {
        "id": "5sNAQjAgr9el"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will be learning a model to map rows of input variables (X) to an output variable (Y), which is often summarized as (Y=F(X).The variables can be summarized as follows:\n",
        "\n",
        "Input Variables (X):\n",
        "\n",
        "1. Number of times pregnant\n",
        "2. Plasma glucose concentration at 2 hours in an oral glucose tolerance test\n",
        "3. Diastolic blood pressure (mm Hg)\n",
        "4. Triceps skin fold thickness (mm)\n",
        "5. 2-hour serum insulin (μIU/ml)\n",
        "6. Body mass index (weight in kg/(height in m)2)\n",
        "7. Diabetes pedigree function\n",
        "8. Age (years)\n",
        "\n",
        "Output Variables (Y):\n",
        "\n",
        "Class label (0 or 1)\n",
        "Once the CSV file is loaded into memory, you can split the columns of data into input and output variables.\n",
        "\n",
        "The data will be stored in a 2D array where the first dimension is rows and the second dimension is columns, e.g., (rows, columns). You can split the array into two arrays by selecting subsets of columns using the standard NumPy slice operator “:“. You can select the first eight columns from index 0 to index 7 via the slice 0:8. You can then select the output column (the 9th variable) via index 8."
      ],
      "metadata": {
        "id": "XOjKNdFytXVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#To convert, we create a tensor out of NumPy arrays:\n",
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)"
      ],
      "metadata": {
        "id": "G6rl5In7sflW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the Model\n",
        "\n",
        "There are indeed, two ways to define a model in PyTorch. The goal is to make it like a function that takes an input and returns an output.\n",
        "\n",
        "A model can be defined as a sequence of layers. You create a Sequential model with the layers listed out. The first thing to get this right is to ensure the first layer has the correct number of input features. In this example, you can specify the input dimension to 8 for the eight input variables as one vector.\n"
      ],
      "metadata": {
        "id": "wJVPxQRJs8Yp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(8, 12),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(12, 8),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(8, 1),\n",
        "    nn.Sigmoid())"
      ],
      "metadata": {
        "id": "bhhivy9YvWsk"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_F3ZgmiEwoaa",
        "outputId": "ef1c1d7b-6b4f-4c02-8d86-929fea0704be"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=8, out_features=12, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=12, out_features=8, bias=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=8, out_features=1, bias=True)\n",
            "  (5): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "...\n",
        "\n",
        "class PimaClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.hidden1 = nn.Linear(8, 12)\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.hidden2 = nn.Linear(12, 8)\n",
        "        self.act2 = nn.ReLU()\n",
        "        self.output = nn.Linear(8, 1)\n",
        "        self.act_output = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.act1(self.hidden1(x))\n",
        "        x = self.act2(self.hidden2(x))\n",
        "        x = self.act_output(self.output(x))\n",
        "        return x\n",
        "\n",
        "model = PimaClassifier()\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpYCgWAqyPes",
        "outputId": "bbc0fe87-650e-4e64-bfa5-55c70a5e4bcc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PimaClassifier(\n",
            "  (hidden1): Linear(in_features=8, out_features=12, bias=True)\n",
            "  (act1): ReLU()\n",
            "  (hidden2): Linear(in_features=12, out_features=8, bias=True)\n",
            "  (act2): ReLU()\n",
            "  (output): Linear(in_features=8, out_features=1, bias=True)\n",
            "  (act_output): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation for Training\n",
        "A model defined is ready for training but you need to specify what is the goal of the training. In this example, the data has the input features X and the output label Y. You want the neural network model to produce an output that is as close to Y as possible. Training a network means to find the best set of weights to map inputs to outputs in your dataset. The metric to measure the prediction’s distance to y is the loss function. In this example, you should use binary cross entropy because it is a binary classification problem.\n",
        "\n",
        "Once you decided on the loss function, you also need an optimizer. Optimizer is the algorithm you use to adjust the model weights progressively to produce a better output. There are many optimizers to choose from and in this example, Adam is used. This is a popular version of gradient descent that can automatically tunes itself and gives good results in a wide range of problems."
      ],
      "metadata": {
        "id": "1C6IZurGyf0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn   = nn.BCELoss()  # binary cross entropy\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "OkR2lDmfyixc"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The optimizer usually has some configuration parameters. Most notably, the learning rate lr. But all optimizers needs to know what to optimize. Therefore we pass on model.parameters(), which is a generator of all parameters from the model we created."
      ],
      "metadata": {
        "id": "0vNetqm4zOPL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training a model"
      ],
      "metadata": {
        "id": "AauBrW9F29nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 100\n",
        "batch_size = 10\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for i in range(0, len(X), batch_size):\n",
        "        Xbatch = X[i:i+batch_size]\n",
        "        y_pred = model(Xbatch)\n",
        "        ybatch = y[i:i+batch_size]\n",
        "        loss = loss_fn(y_pred, ybatch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Finished epoch {epoch}, latest loss {loss}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pi6LkFoi3C_6",
        "outputId": "5e7966ea-db14-41e4-86b8-7fa75e957006"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished epoch 0, latest loss 0.7592817544937134\n",
            "Finished epoch 1, latest loss 0.6184865236282349\n",
            "Finished epoch 2, latest loss 0.5555864572525024\n",
            "Finished epoch 3, latest loss 0.5279662013053894\n",
            "Finished epoch 4, latest loss 0.5120716691017151\n",
            "Finished epoch 5, latest loss 0.4971652925014496\n",
            "Finished epoch 6, latest loss 0.5073463320732117\n",
            "Finished epoch 7, latest loss 0.507996678352356\n",
            "Finished epoch 8, latest loss 0.5129403471946716\n",
            "Finished epoch 9, latest loss 0.5150443911552429\n",
            "Finished epoch 10, latest loss 0.5177299380302429\n",
            "Finished epoch 11, latest loss 0.5183866620063782\n",
            "Finished epoch 12, latest loss 0.5203548073768616\n",
            "Finished epoch 13, latest loss 0.5215805768966675\n",
            "Finished epoch 14, latest loss 0.5250221490859985\n",
            "Finished epoch 15, latest loss 0.5228061079978943\n",
            "Finished epoch 16, latest loss 0.5166512727737427\n",
            "Finished epoch 17, latest loss 0.5182601809501648\n",
            "Finished epoch 18, latest loss 0.5157771110534668\n",
            "Finished epoch 19, latest loss 0.5139381289482117\n",
            "Finished epoch 20, latest loss 0.5152034759521484\n",
            "Finished epoch 21, latest loss 0.5170390605926514\n",
            "Finished epoch 22, latest loss 0.5137003064155579\n",
            "Finished epoch 23, latest loss 0.5113345384597778\n",
            "Finished epoch 24, latest loss 0.5118831992149353\n",
            "Finished epoch 25, latest loss 0.5122328400611877\n",
            "Finished epoch 26, latest loss 0.5104318261146545\n",
            "Finished epoch 27, latest loss 0.502461850643158\n",
            "Finished epoch 28, latest loss 0.5030707716941833\n",
            "Finished epoch 29, latest loss 0.503730833530426\n",
            "Finished epoch 30, latest loss 0.4278198182582855\n",
            "Finished epoch 31, latest loss 0.48390451073646545\n",
            "Finished epoch 32, latest loss 0.48130929470062256\n",
            "Finished epoch 33, latest loss 0.4887949824333191\n",
            "Finished epoch 34, latest loss 0.4847385883331299\n",
            "Finished epoch 35, latest loss 0.4758804738521576\n",
            "Finished epoch 36, latest loss 0.472380667924881\n",
            "Finished epoch 37, latest loss 0.4711330235004425\n",
            "Finished epoch 38, latest loss 0.45951688289642334\n",
            "Finished epoch 39, latest loss 0.4620078206062317\n",
            "Finished epoch 40, latest loss 0.4597174823284149\n",
            "Finished epoch 41, latest loss 0.456663578748703\n",
            "Finished epoch 42, latest loss 0.4574539065361023\n",
            "Finished epoch 43, latest loss 0.4514879584312439\n",
            "Finished epoch 44, latest loss 0.4458072781562805\n",
            "Finished epoch 45, latest loss 0.43087121844291687\n",
            "Finished epoch 46, latest loss 0.440418541431427\n",
            "Finished epoch 47, latest loss 0.4323725402355194\n",
            "Finished epoch 48, latest loss 0.42335471510887146\n",
            "Finished epoch 49, latest loss 0.4164940416812897\n",
            "Finished epoch 50, latest loss 0.41843295097351074\n",
            "Finished epoch 51, latest loss 0.39998751878738403\n",
            "Finished epoch 52, latest loss 0.4285951852798462\n",
            "Finished epoch 53, latest loss 0.405089408159256\n",
            "Finished epoch 54, latest loss 0.41827359795570374\n",
            "Finished epoch 55, latest loss 0.4100707471370697\n",
            "Finished epoch 56, latest loss 0.4104371964931488\n",
            "Finished epoch 57, latest loss 0.40359172224998474\n",
            "Finished epoch 58, latest loss 0.4036688208580017\n",
            "Finished epoch 59, latest loss 0.40031346678733826\n",
            "Finished epoch 60, latest loss 0.4131573736667633\n",
            "Finished epoch 61, latest loss 0.4020349681377411\n",
            "Finished epoch 62, latest loss 0.39466002583503723\n",
            "Finished epoch 63, latest loss 0.3956942856311798\n",
            "Finished epoch 64, latest loss 0.38816866278648376\n",
            "Finished epoch 65, latest loss 0.39978134632110596\n",
            "Finished epoch 66, latest loss 0.39162391424179077\n",
            "Finished epoch 67, latest loss 0.39188504219055176\n",
            "Finished epoch 68, latest loss 0.390142023563385\n",
            "Finished epoch 69, latest loss 0.38506874442100525\n",
            "Finished epoch 70, latest loss 0.3850889205932617\n",
            "Finished epoch 71, latest loss 0.38414233922958374\n",
            "Finished epoch 72, latest loss 0.3763638734817505\n",
            "Finished epoch 73, latest loss 0.38644689321517944\n",
            "Finished epoch 74, latest loss 0.37646451592445374\n",
            "Finished epoch 75, latest loss 0.3788342773914337\n",
            "Finished epoch 76, latest loss 0.3808736503124237\n",
            "Finished epoch 77, latest loss 0.3741614520549774\n",
            "Finished epoch 78, latest loss 0.38067424297332764\n",
            "Finished epoch 79, latest loss 0.37309375405311584\n",
            "Finished epoch 80, latest loss 0.3742927312850952\n",
            "Finished epoch 81, latest loss 0.3703949451446533\n",
            "Finished epoch 82, latest loss 0.3701523244380951\n",
            "Finished epoch 83, latest loss 0.3744378089904785\n",
            "Finished epoch 84, latest loss 0.3665546476840973\n",
            "Finished epoch 85, latest loss 0.37304413318634033\n",
            "Finished epoch 86, latest loss 0.3703189492225647\n",
            "Finished epoch 87, latest loss 0.3641386330127716\n",
            "Finished epoch 88, latest loss 0.36136800050735474\n",
            "Finished epoch 89, latest loss 0.3602624833583832\n",
            "Finished epoch 90, latest loss 0.3602657616138458\n",
            "Finished epoch 91, latest loss 0.36103177070617676\n",
            "Finished epoch 92, latest loss 0.3612584173679352\n",
            "Finished epoch 93, latest loss 0.36435145139694214\n",
            "Finished epoch 94, latest loss 0.3684465289115906\n",
            "Finished epoch 95, latest loss 0.34784919023513794\n",
            "Finished epoch 96, latest loss 0.3574201464653015\n",
            "Finished epoch 97, latest loss 0.3476763367652893\n",
            "Finished epoch 98, latest loss 0.34498363733291626\n",
            "Finished epoch 99, latest loss 0.3445320725440979\n"
          ]
        }
      ]
    }
  ]
}